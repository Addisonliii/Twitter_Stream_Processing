{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zw2497/Twitter_Stream_Processing/blob/master/PySpark_Structured_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvD4HBMi0ohY"
   },
   "source": [
    "# Install Java, Spark, and Findspark\n",
    "This installs Apache Spark 2.4.0, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fUhBhrGmyAvs",
    "outputId": "8696e6f2-1e81-48d1-f63d-c289857c58af"
   },
   "outputs": [],
   "source": [
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://www-us.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-2.4.2-bin-hadoop2.7.tgz\n",
    "# !pip -q install findspark\n",
    "\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"./spark-2.4.2-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4Kjvk_h1AHl"
   },
   "source": [
    "#### Set Environment Variables\n",
    "Set the locations where Spark and Java are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, get_json_object\n",
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local --packages org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.2 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('FATAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"35.243.144.79:9092\") \\\n",
    "  .option(\"subscribe\", \"tweepyv1\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"select decode(value, 'utf-8') as value, timestamp \n",
    "                  from raw\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(x):\n",
    "    blob = TextBlob(x)\n",
    "    s = []\n",
    "    for sentence in blob.sentences:\n",
    "        s.append(sentence.sentiment.polarity)\n",
    "    return sum(s)/len(s)\n",
    "\n",
    "sent = udf(polarity, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('timestamp',\\\n",
    "               get_json_object('value', '$.entities.hashtags[0].text').alias(\"hashtag\"), \\\n",
    "               sent(get_json_object('value', '$.text')).alias(\"sentiment\"))\n",
    "df = df.filter(df.hashtag.isNotNull())\n",
    "df.createOrReplaceTempView(\"datas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "select distinct hashtag, count(*) as count, avg(sentiment) as sentiment, window(timestamp, \"600 seconds\", \"60 seconds\") as key\n",
    "from datas\n",
    "group by hashtag, window(timestamp, \"600 seconds\", \"60 seconds\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df.writeStream.outputMode(\"complete\").queryName(\"test\").format(\"memory\").option(\"truncate\", \"False\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = spark.table(\"test\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtag</th>\n",
       "      <th>count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:04:00, 2019-05-03 05:14:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:10:00, 2019-05-03 05:20:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:09:00, 2019-05-03 05:19:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:11:00, 2019-05-03 05:21:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>3</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:06:00, 2019-05-03 05:16:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>LTBU</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:04:00, 2019-05-03 05:14:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>StarOnFox</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>KCAL9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>stemcells</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>KCAL9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>stemcells</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:10:00, 2019-05-03 05:20:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>StarOnFox</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:11:00, 2019-05-03 05:21:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>KCAL9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:11:00, 2019-05-03 05:21:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>stemcells</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:04:00, 2019-05-03 05:14:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>LTBU</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:06:00, 2019-05-03 05:16:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>LTBU</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>KCAL9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:10:00, 2019-05-03 05:20:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>HowToNotBeAnAwfulHuman</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>stemcells</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>HowToNotBeAnAwfulHuman</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>stemcells</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:06:00, 2019-05-03 05:16:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>nothingisordinary</td>\n",
       "      <td>2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>LTBU</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:10:00, 2019-05-03 05:20:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>StarOnFox</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>jpreads</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>WeAreMetro</td>\n",
       "      <td>1</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>(2019-05-03 05:03:00, 2019-05-03 05:13:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>sickofbernie</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:09:00, 2019-05-03 05:19:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>HowToNotBeAnAwfulHuman</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>IMPERFECTLIFE</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>PP19000142859</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:13:00, 2019-05-03 05:23:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>billingshomes</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063636</td>\n",
       "      <td>(2019-05-03 05:03:00, 2019-05-03 05:13:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>photoaday</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>SeeWhatIDidThere</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>albumreleaseparty</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044286</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>bearnationbaseball</td>\n",
       "      <td>1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>ChargingBull</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:07:00, 2019-05-03 05:17:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>hgmf19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>HelpBringChristianHome</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>(2019-05-03 05:04:00, 2019-05-03 05:14:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>blackgirlmagic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>LanaiCity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>blackops4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:05:00, 2019-05-03 05:15:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>largerthanlife</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>(2019-05-03 05:04:00, 2019-05-03 05:14:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>DEVELOP_E12_ISB</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>(2019-05-03 05:12:00, 2019-05-03 05:22:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>healthyliving</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:09:00, 2019-05-03 05:19:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>whiteboys</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:09:00, 2019-05-03 05:19:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>DeliriousArmy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>(2019-05-03 05:06:00, 2019-05-03 05:16:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>earthquake</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>stemcells</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>(2019-05-03 05:04:00, 2019-05-03 05:14:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>girlsgottaeat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>(2019-05-03 05:10:00, 2019-05-03 05:20:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>werk</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>WeAreMetro</td>\n",
       "      <td>1</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>(2019-05-03 05:08:00, 2019-05-03 05:18:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:03:00, 2019-05-03 05:13:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>thefarmhousebrewery</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:03:00, 2019-05-03 05:13:00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>panera</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(2019-05-03 05:11:00, 2019-05-03 05:21:00)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1520 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hashtag  count  sentiment  \\\n",
       "612        nothingisordinary      3   0.100000   \n",
       "4          nothingisordinary      3   0.100000   \n",
       "1114       nothingisordinary      3   0.100000   \n",
       "824        nothingisordinary      3   0.100000   \n",
       "1492       nothingisordinary      3   0.100000   \n",
       "1377       nothingisordinary      3   0.100000   \n",
       "99         nothingisordinary      3   0.100000   \n",
       "803        nothingisordinary      3   0.100000   \n",
       "1077       nothingisordinary      3   0.100000   \n",
       "1129                    LTBU      2   0.000000   \n",
       "866        nothingisordinary      2   0.100000   \n",
       "1175               StarOnFox      2   0.000000   \n",
       "156                    KCAL9      2   0.000000   \n",
       "449                stemcells      2   0.100000   \n",
       "1254                   KCAL9      2   0.000000   \n",
       "1372               stemcells      2   0.100000   \n",
       "187                StarOnFox      2   0.000000   \n",
       "636        nothingisordinary      2   0.100000   \n",
       "487                    KCAL9      2   0.000000   \n",
       "1284               stemcells      2   0.100000   \n",
       "1054                    LTBU      2   0.000000   \n",
       "637                     LTBU      2   0.000000   \n",
       "686                    KCAL9      2   0.000000   \n",
       "178   HowToNotBeAnAwfulHuman      2   0.000000   \n",
       "1178               stemcells      2   0.100000   \n",
       "738   HowToNotBeAnAwfulHuman      2   0.000000   \n",
       "1197               stemcells      2   0.100000   \n",
       "1196       nothingisordinary      2   0.100000   \n",
       "85                      LTBU      2   0.000000   \n",
       "528                StarOnFox      2   0.000000   \n",
       "...                      ...    ...        ...   \n",
       "494                  jpreads      1   0.100000   \n",
       "493               WeAreMetro      1   0.263889   \n",
       "492             sickofbernie      1   0.000000   \n",
       "491   HowToNotBeAnAwfulHuman      1   0.000000   \n",
       "490            IMPERFECTLIFE      1   0.500000   \n",
       "489            PP19000142859      1   0.000000   \n",
       "508            billingshomes      1   0.063636   \n",
       "510                photoaday      1   0.000000   \n",
       "533         SeeWhatIDidThere      1   0.000000   \n",
       "511        albumreleaseparty      1   0.044286   \n",
       "532       bearnationbaseball      1   0.133333   \n",
       "531             ChargingBull      1   0.000000   \n",
       "530                   hgmf19      1   0.000000   \n",
       "529   HelpBringChristianHome      1   0.233333   \n",
       "527           blackgirlmagic      1   0.400000   \n",
       "526                LanaiCity      1   0.000000   \n",
       "525                blackops4      1   0.000000   \n",
       "524           largerthanlife      1   0.500000   \n",
       "523          DEVELOP_E12_ISB      1  -0.125000   \n",
       "522            healthyliving      1   0.000000   \n",
       "520                whiteboys      1   0.000000   \n",
       "519            DeliriousArmy      1   0.250000   \n",
       "518               earthquake      1   0.000000   \n",
       "517                stemcells      1   0.100000   \n",
       "516            girlsgottaeat      1   0.318182   \n",
       "515                     werk      1   0.250000   \n",
       "514               WeAreMetro      1   0.263889   \n",
       "513                    Idaho      1   0.000000   \n",
       "512      thefarmhousebrewery      1   0.000000   \n",
       "1519                  panera      1   0.000000   \n",
       "\n",
       "                                             key  \n",
       "612   (2019-05-03 05:04:00, 2019-05-03 05:14:00)  \n",
       "4     (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "1114  (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "824   (2019-05-03 05:10:00, 2019-05-03 05:20:00)  \n",
       "1492  (2019-05-03 05:09:00, 2019-05-03 05:19:00)  \n",
       "1377  (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "99    (2019-05-03 05:11:00, 2019-05-03 05:21:00)  \n",
       "803   (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "1077  (2019-05-03 05:06:00, 2019-05-03 05:16:00)  \n",
       "1129  (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "866   (2019-05-03 05:04:00, 2019-05-03 05:14:00)  \n",
       "1175  (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "156   (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "449   (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "1254  (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "1372  (2019-05-03 05:10:00, 2019-05-03 05:20:00)  \n",
       "187   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "636   (2019-05-03 05:11:00, 2019-05-03 05:21:00)  \n",
       "487   (2019-05-03 05:11:00, 2019-05-03 05:21:00)  \n",
       "1284  (2019-05-03 05:04:00, 2019-05-03 05:14:00)  \n",
       "1054  (2019-05-03 05:06:00, 2019-05-03 05:16:00)  \n",
       "637   (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "686   (2019-05-03 05:10:00, 2019-05-03 05:20:00)  \n",
       "178   (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "1178  (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "738   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "1197  (2019-05-03 05:06:00, 2019-05-03 05:16:00)  \n",
       "1196  (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "85    (2019-05-03 05:10:00, 2019-05-03 05:20:00)  \n",
       "528   (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "...                                          ...  \n",
       "494   (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "493   (2019-05-03 05:03:00, 2019-05-03 05:13:00)  \n",
       "492   (2019-05-03 05:09:00, 2019-05-03 05:19:00)  \n",
       "491   (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "490   (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "489   (2019-05-03 05:13:00, 2019-05-03 05:23:00)  \n",
       "508   (2019-05-03 05:03:00, 2019-05-03 05:13:00)  \n",
       "510   (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "533   (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "511   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "532   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "531   (2019-05-03 05:07:00, 2019-05-03 05:17:00)  \n",
       "530   (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "529   (2019-05-03 05:04:00, 2019-05-03 05:14:00)  \n",
       "527   (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "526   (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "525   (2019-05-03 05:05:00, 2019-05-03 05:15:00)  \n",
       "524   (2019-05-03 05:04:00, 2019-05-03 05:14:00)  \n",
       "523   (2019-05-03 05:12:00, 2019-05-03 05:22:00)  \n",
       "522   (2019-05-03 05:09:00, 2019-05-03 05:19:00)  \n",
       "520   (2019-05-03 05:09:00, 2019-05-03 05:19:00)  \n",
       "519   (2019-05-03 05:06:00, 2019-05-03 05:16:00)  \n",
       "518   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "517   (2019-05-03 05:04:00, 2019-05-03 05:14:00)  \n",
       "516   (2019-05-03 05:10:00, 2019-05-03 05:20:00)  \n",
       "515   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "514   (2019-05-03 05:08:00, 2019-05-03 05:18:00)  \n",
       "513   (2019-05-03 05:03:00, 2019-05-03 05:13:00)  \n",
       "512   (2019-05-03 05:03:00, 2019-05-03 05:13:00)  \n",
       "1519  (2019-05-03 05:11:00, 2019-05-03 05:21:00)  \n",
       "\n",
       "[1520 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sort_values(by='count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwU28K5f1H3P"
   },
   "source": [
    "# Start a SparkSession\n",
    "This will start a local Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3NyDBHg6HKjw",
    "outputId": "ab29e74c-6a10-4c13-f496-c4e69bbc99e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./app.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('FATAL')\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"35.243.144.79:9092\") \\\n",
    "  .option(\"subscribe\", \"tweepyv1\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .option(\"failOnDataLoss\", \"false\") \\\n",
    "  .load()\n",
    "\n",
    "\"\"\"\n",
    "UDF\n",
    "\"\"\"\n",
    "def polarity(x):\n",
    "    blob = TextBlob(x)\n",
    "    s = []\n",
    "    for sentence in blob.sentences:\n",
    "        s.append(sentence.sentiment.polarity)\n",
    "    return sum(s)/len(s)\n",
    "\n",
    "\"\"\"\n",
    "type cast\n",
    "\"\"\"\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "print(\"Streaming: \", df.isStreaming)\n",
    "print(\"Start receiving ... ...\")\n",
    "df.createOrReplaceTempView(\"raw\")\n",
    "\n",
    "\"\"\"\n",
    "decode\n",
    "\"\"\"\n",
    "df = spark.sql(\"\"\"select decode(value, 'utf-8') as value, timestamp \n",
    "                  from raw\"\"\");\n",
    "df.createGlobalTempView(\"decode_data\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "add sentiment\n",
    "\"\"\"\n",
    "dfsentiment = spark.sql(\"\"\"\n",
    "select polarity(get_json_object(value, '$.text')) as sentiment, value, timestamp\n",
    "from decode_data\n",
    "\"\"\");\n",
    "\n",
    "dfsentiment.createGlobalTempView(\"addsentiment\");\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# client Application\n",
    "# \"\"\"\n",
    "# dfclient = spark.sql(\"\"\"select get_json_object(value, '$.source') as key, count(*) as value\n",
    "#                         from decode\n",
    "#                         group by get_json_object(value, '$.source')\n",
    "#                         order by value DESC\"\"\");\n",
    "\n",
    "\"\"\"\n",
    "get hashtag\n",
    "\"\"\"\n",
    "dfhashtag = spark.sql(\"\"\"\n",
    "select get_json_object(value, '$.entities.hashtags[0].text') as hashtag, timestamp, sentiment\n",
    "from addsentiment \n",
    "where length(get_json_object(value, '$.entities.hashtags[0].text')) > 2\n",
    "\"\"\");\n",
    "dfhashtag.createOrReplaceTempView(\"tag\");\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "compute window\n",
    "\"\"\"\n",
    "# dffast = spark.sql(\"\"\"\n",
    "# select concat_ws(' ', hashtag, string(count(*)), avg(sentiment)) as value, now() as key\n",
    "# from tag\n",
    "# group by window(timestamp, \"10 seconds\", \"5 seconds\"), hashtag\n",
    "# \"\"\")\n",
    "\n",
    "dfslow = spark.sql(\"\"\"\n",
    "select concat_ws(' ', hashtag, string(count(*)), avg(sentiment)) as value, now() as key\n",
    "from tag\n",
    "group by window(timestamp, \"600 seconds\", \"60 seconds\"), hashtag\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Write to Console\n",
    "\"\"\"\n",
    "query1 = dfslow \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"False\") \\\n",
    "    .option(\"checkpointLocation\", \"./logslow\") \\\n",
    "    .start()\n",
    "\n",
    "# query2 = dffast \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", \"False\") \\\n",
    "#     .option(\"checkpointLocation\", \"./logfast\") \\\n",
    "#     .start()\n",
    "\n",
    "# query3 = dfclient \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"truncate\", \"False\") \\\n",
    "#     .option(\"checkpointLocation\", \"./logclient\") \\\n",
    "#     .start()\n",
    "\n",
    "\"\"\"\n",
    "Write to kafka\n",
    "\"\"\"\n",
    "# query1 = dffast \\\n",
    "#   .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "#   .writeStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .outputMode(\"update\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"35.243.144.79:9092\") \\\n",
    "#   .option(\"topic\", \"topk1\") \\\n",
    "#   .option(\"checkpointLocation\", \"./logfast\") \\\n",
    "#   .start()\n",
    "\n",
    "# query2 = dfslow \\\n",
    "#   .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "#   .writeStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .outputMode(\"update\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"35.243.144.79:9092\") \\\n",
    "#   .option(\"topic\", \"topk1\") \\\n",
    "#   .option(\"checkpointLocation\", \"./logslow\") \\\n",
    "#   .start()\n",
    "\n",
    "# query3 = dfclient \\\n",
    "#   .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "#   .writeStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .outputMode(\"complete\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"35.243.144.79:9092\") \\\n",
    "#   .option(\"topic\", \"source\") \\\n",
    "#   .option(\"checkpointLocation\", \"./logclient\") \\\n",
    "#   .start()\n",
    "\n",
    "\n",
    "\n",
    "query1.awaitTermination()\n",
    "# query2.awaitTermination()\n",
    "# query3.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1392
    },
    "colab_type": "code",
    "id": "kSnvAWZ_HKge",
    "outputId": "984eb444-16d6-4c4e-9401-e8f7d678128b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-2.4.2-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ef1a3b3f-8f6c-4add-8eaa-401ad719eb6f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;2.4.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.0.0 in central\n",
      "\tfound org.lz4#lz4-java;1.4.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 361ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.kafka#kafka-clients;2.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;2.4.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.4.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ef1a3b3f-8f6c-4add-8eaa-401ad719eb6f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/7ms)\n",
      "19/05/02 23:43:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/05/02 23:43:55 INFO SparkContext: Running Spark version 2.4.2\n",
      "19/05/02 23:43:55 INFO SparkContext: Submitted application: StructuredNetworkWordCount\n",
      "19/05/02 23:43:55 INFO SecurityManager: Changing view acls to: jovyan\n",
      "19/05/02 23:43:55 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "19/05/02 23:43:55 INFO SecurityManager: Changing view acls groups to: \n",
      "19/05/02 23:43:55 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/05/02 23:43:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "19/05/02 23:43:56 INFO Utils: Successfully started service 'sparkDriver' on port 37049.\n",
      "19/05/02 23:43:56 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/05/02 23:43:56 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/05/02 23:43:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/05/02 23:43:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/05/02 23:43:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-19015bd3-4955-400b-aec1-c634b00f36b0\n",
      "19/05/02 23:43:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/05/02 23:43:56 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/05/02 23:43:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "19/05/02 23:43:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ae83f739724c:4040\n",
      "19/05/02 23:43:56 INFO SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-2.4.2.jar at spark://ae83f739724c:37049/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-2.4.2.jar with timestamp 1556840636717\n",
      "19/05/02 23:43:56 INFO SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar at spark://ae83f739724c:37049/jars/org.apache.kafka_kafka-clients-2.0.0.jar with timestamp 1556840636718\n",
      "19/05/02 23:43:56 INFO SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ae83f739724c:37049/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1556840636718\n",
      "19/05/02 23:43:56 INFO SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar at spark://ae83f739724c:37049/jars/org.lz4_lz4-java-1.4.0.jar with timestamp 1556840636718\n",
      "19/05/02 23:43:56 INFO SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar at spark://ae83f739724c:37049/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1556840636718\n",
      "19/05/02 23:43:56 INFO SparkContext: Added JAR file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://ae83f739724c:37049/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1556840636719\n",
      "19/05/02 23:43:56 INFO SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-2.4.2.jar at file:///home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-2.4.2.jar with timestamp 1556840636746\n",
      "19/05/02 23:43:56 INFO Utils: Copying /home/jovyan/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-2.4.2.jar to /tmp/spark-6b8e2fc6-12c9-4b85-a036-7054d00ea43c/userFiles-39c42d84-ae3c-4d7e-8e64-b9ae06b74281/org.apache.spark_spark-sql-kafka-0-10_2.12-2.4.2.jar\n",
      "19/05/02 23:43:56 INFO SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar at file:///home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar with timestamp 1556840636763\n",
      "19/05/02 23:43:56 INFO Utils: Copying /home/jovyan/.ivy2/jars/org.apache.kafka_kafka-clients-2.0.0.jar to /tmp/spark-6b8e2fc6-12c9-4b85-a036-7054d00ea43c/userFiles-39c42d84-ae3c-4d7e-8e64-b9ae06b74281/org.apache.kafka_kafka-clients-2.0.0.jar\n",
      "19/05/02 23:43:56 INFO SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1556840636782\n",
      "19/05/02 23:43:56 INFO Utils: Copying /home/jovyan/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6b8e2fc6-12c9-4b85-a036-7054d00ea43c/userFiles-39c42d84-ae3c-4d7e-8e64-b9ae06b74281/org.spark-project.spark_unused-1.0.0.jar\n",
      "19/05/02 23:43:56 INFO SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar at file:///home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar with timestamp 1556840636786\n",
      "19/05/02 23:43:56 INFO Utils: Copying /home/jovyan/.ivy2/jars/org.lz4_lz4-java-1.4.0.jar to /tmp/spark-6b8e2fc6-12c9-4b85-a036-7054d00ea43c/userFiles-39c42d84-ae3c-4d7e-8e64-b9ae06b74281/org.lz4_lz4-java-1.4.0.jar\n",
      "19/05/02 23:43:56 INFO SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar at file:///home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar with timestamp 1556840636796\n",
      "19/05/02 23:43:56 INFO Utils: Copying /home/jovyan/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.3.jar to /tmp/spark-6b8e2fc6-12c9-4b85-a036-7054d00ea43c/userFiles-39c42d84-ae3c-4d7e-8e64-b9ae06b74281/org.xerial.snappy_snappy-java-1.1.7.3.jar\n",
      "19/05/02 23:43:56 INFO SparkContext: Added file file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1556840636804\n",
      "19/05/02 23:43:56 INFO Utils: Copying /home/jovyan/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-6b8e2fc6-12c9-4b85-a036-7054d00ea43c/userFiles-39c42d84-ae3c-4d7e-8e64-b9ae06b74281/org.slf4j_slf4j-api-1.7.16.jar\n",
      "19/05/02 23:43:56 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/05/02 23:43:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40065.\n",
      "19/05/02 23:43:57 INFO NettyBlockTransferService: Server created on ae83f739724c:40065\n",
      "19/05/02 23:43:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/05/02 23:43:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ae83f739724c, 40065, None)\n",
      "19/05/02 23:43:57 INFO BlockManagerMasterEndpoint: Registering block manager ae83f739724c:40065 with 366.3 MB RAM, BlockManagerId(driver, ae83f739724c, 40065, None)\n",
      "19/05/02 23:43:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ae83f739724c, 40065, None)\n",
      "19/05/02 23:43:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ae83f739724c, 40065, None)\n",
      "19/05/02 23:43:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/jovyan/work/Twitter_Stream_Processing/spark-warehouse/').\n",
      "19/05/02 23:43:57 INFO SharedState: Warehouse path is 'file:/home/jovyan/work/Twitter_Stream_Processing/spark-warehouse/'.\n",
      "19/05/02 23:43:58 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "Streaming:  True\n",
      "Start receiving ... ...\n",
      "#\n",
      "# A fatal error has been detected by the Java Runtime Environment:\n",
      "#\n",
      "#  SIGSEGV (0xb) at pc=0x00007f27b87e5033, pid=307, tid=0x00007f272c8d9700\n",
      "#\n",
      "# JRE version: OpenJDK Runtime Environment (8.0_191-b12) (build 1.8.0_191-8u191-b12-2ubuntu0.18.04.1-b12)\n",
      "# Java VM: OpenJDK 64-Bit Server VM (25.191-b12 mixed mode linux-amd64 compressed oops)\n",
      "# Problematic frame:\n",
      "# j  org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithKeys_0$(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$GeneratedIteratorForCodegenStage5;)V+7\n",
      "#\n",
      "# Core dump written. Default location: /home/jovyan/work/Twitter_Stream_Processing/core or core.307\n",
      "#\n",
      "# An error report file with more information is saved as:\n",
      "# /home/jovyan/work/Twitter_Stream_Processing/hs_err_pid307.log\n",
      "[thread 139806282503936 also had an error]\n",
      "#\n",
      "# If you would like to submit a bug report, please visit:\n",
      "#   http://bugreport.java.com/bugreport/crash.jsp\n",
      "#\n",
      "Aborted (core dumped)\n"
     ]
    }
   ],
   "source": [
    "!./spark-2.4.2-bin-hadoop2.7/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.2 app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQwC6yY5NxE8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PySpark_Structured_Streaming.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
